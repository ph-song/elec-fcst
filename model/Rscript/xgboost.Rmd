---
title: "Untitled"
output: html_document
date: "2023-09-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(forecast)
library(fpp3)
library(xgboost)

data <- read.csv("cleaned_actual.csv")

# Assuming your date column is named 'Time' and setting it as a date column
data$Time <- as.POSIXct(data$Time, format="%Y-%m-%d %H:%M", tz="UTC")

# Creating 168-lags (if not created before)
data <- data %>%
  arrange(Time) %>%
  mutate(across(-Time, list(lag_168 = ~ lag(.x, 168)), .names = "lag168_{col}"))

# Remove rows with NAs
data <- data %>% drop_na()

library(dplyr)

# Find the most recent date in the dataset
most_recent_date <- max(data$Time)

# Calculate the date 3 years before the most recent date
start_date <- as.POSIXct(most_recent_date - lubridate::years(3), tz="UTC")

# Filter the data to include only the last 3 years
data <- data %>% filter(Time >= start_date)

# Now data_last_3_years contains only the most recent 3 years of data

```

```{r}
set.seed(666)
sample_size <- nrow(data)
train_index <- 1:(sample_size * 0.8)
val_index <- (sample_size * 0.8 + 1):(sample_size * 0.9)
test_index <- (sample_size * 0.9 + 1):sample_size

train_data <- data[train_index, ]
val_data <- data[val_index, ]
test_data <- data[test_index, ]

```

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, grepl("lag168_", names(train_data))]), label = train_data$Load..kW.)
dval <- xgb.DMatrix(data = as.matrix(val_data[, grepl("lag168_", names(val_data))]), label = val_data$Load..kW.)

# Define the grid of parameters to search over
param_grid <- expand.grid(
  max_depth = seq(2, 10, 2), 
  eta = seq(0.1, 0.3, by = 0.1), 
  subsample = seq(0.5, 1, by = 0.1),
  colsample_bytree = seq(0.5, 1, by = 0.1)
)

best_model <- NULL
best_params <- NULL
best_mae <- Inf

# Loop over all rows in param_grid
for(i in seq(nrow(param_grid))) {
  
  params <- list(
    max_depth = param_grid$max_depth[i], 
    eta = param_grid$eta[i], 
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i],
    objective = "reg:squarederror", 
    eval_metric = "mae"
    #tree_method = "gpu_hist", # Enable GPU acceleration
    #gpu_id = 0 # ID of the GPU to use
  )
  
  watchlist <- list(train = dtrain, val = dval)
  
  set.seed(123)
  model <- xgb.train(params, dtrain, nrounds = 100, watchlist, early_stopping_rounds = 10, verbose = 0)
  
  # Predict on validation set and calculate MAE
  val_preds <- predict(model, dval)
  mae <- mean(abs(val_data$Load..kW. - val_preds))
  
  # If this model has the best MAE so far, update best_model and best_mae
  if(mae < best_mae) {
    best_model <- model
    best_params <- params
    best_mae <- mae
  }
}


# best_model now contains the model with the best performance on the validation set
print(best_params)
print(best_mae)

best_params <- list(
    max_depth = best_params$max_depth, 
    eta = best_params$eta, 
    subsample = best_params$subsample, 
    colsample_bytree = best_params$colsample_bytree, 
    objective = "reg:squarederror", 
    eval_metric = "mae" 
    #tree_method = "gpu_hist", 
    #gpu_id = 0 
)

```

```{r}
library(tidyverse)
library(forecast)
library(fpp3)
feb01 <- read.csv("./data/test/Actuals_Feb 01 8AM.csv")
feb02 <- read.csv("./data/test/Actuals_Feb 02 8AM.csv")
feb03 <- read.csv("./data/test/Actuals_Feb 03 8AM.csv")
feb04 <- read.csv("./data/test/Actuals_Feb 04 8AM.csv")
feb05 <- read.csv("./data/test/Actuals_Feb 05 8AM.csv")
feb06 <- read.csv("./data/test/Actuals_Feb 06 8AM.csv")
feb07 <- read.csv("./data/test/Actuals_Feb 07 8AM.csv")
feb08 <- read.csv("./data/test/Actuals_Feb 08 8AM.csv")
feb09 <- read.csv("./data/test/Actuals_Feb 09 8AM.csv")
feb10 <- read.csv("./data/test/Actuals_Feb 10 8AM.csv")
feb11 <- read.csv("./data/test/Actuals_Feb 11 8AM.csv")
feb12 <- read.csv("./data/test/Actuals_Feb 12 8AM.csv")
feb13 <- read.csv("./data/test/Actuals_Feb 13 8AM.csv")
feb14 <- read.csv("./data/test/Actuals_Feb 14 8AM.csv")
feb15 <- read.csv("./data/test/Actuals_Feb 15 8AM.csv")

# Remove rows with NAs for the January files
list_of_dfs <- list(feb01, feb02, feb03, feb04, feb05, feb06, feb07, feb08, feb09, feb10, feb11, feb12, feb13, feb14, feb15)
list_of_dfs <- lapply(list_of_dfs, function(x) x %>% filter_all(all_vars(!is.na(.))))

training_data <- read.csv("./data/cleaned_actual.csv")

# Set column names for all the jan dataframes
list_of_dfs <- list(feb01, feb02, feb03, feb04, feb05, feb06, feb07, feb08, feb09, feb10, feb11, feb12, feb13, feb14, feb15)

# Extract column names from df1
training_data_colnames <- colnames(training_data)

# Apply df1's column names to all the jan dataframes
list_of_dfs <- lapply(list_of_dfs, function(training_data) {
  colnames(training_data) <- training_data_colnames
  return(training_data)
})

# Convert Time column to POSIXct for all other dataframes
list_of_dfs <- lapply(list_of_dfs, function(x) {
  x$Time <- as.POSIXct(x$Time, format="%Y-%m-%d %H:%M", tz="UTC")
  return(x)
})

test_data <- rbind(do.call(rbind, list_of_dfs))

training_data$Time <- as.POSIXct(training_data$Time, format="%Y-%m-%d %H:%M", tz="UTC")

whole_data <- rbind(training_data, test_data)


whole_data <- whole_data %>%
  arrange(Time) %>%
  mutate(across(-Time, list(lag_168 = ~ lag(.x, 168)), .names = "lag168_{col}"))
```


```{r}
set.seed(666)
library(xgboost)



forecast_xgboost <- function(model, test) {
  dtest <- xgb.DMatrix(data = as.matrix(test[, grepl("lag168_", names(test))]))

  pred <- predict(model, dtest)
  return(pred)
}





# Rolling forecast parameters
roll_size <- 24 # 24 hours
end_of_train_data <- which(whole_data$Time == "2021-01-31 07:00:00")
start_test <- which(whole_data$Time == "2021-01-31 08:00:00")
end_data <- which(whole_data$Time == "2021-02-15 07:00:00")

mae_list <- list()

while((start_test + roll_size - 1) <= end_data) {
  set.seed(666)
  # Subset the training data (last two years from the current testing point)
  train_subset <- whole_data[(end_of_train_data - 2*365*24 + 1):end_of_train_data, ]
  test_subset <- whole_data[start_test:(start_test + roll_size - 1), ]
  
  xgboost_preds <- forecast_xgboost(best_model, test_subset)
  xgboost_mae <- mean(abs(test_subset$Load..kW. - xgboost_preds))
  
  
 
  # Roll forward by 48 hours in the test data
  start_test <- start_test + roll_size
  end_of_train_data <- end_of_train_data + roll_size
  
   mae_list[[paste0("Start_", as.character(whole_data$Time[start_test]))]] <- c( xgboost_mae)
}

# Convert the results list to a data frame
df_mae <- do.call(rbind.data.frame, mae_list)


colnames(df_mae) <- c("XGBoost")

avg_mae <- data.frame(Method = names(colMeans(df_mae)), Average_MAE = as.vector(colMeans(df_mae)))

# Sorting the avg_mae dataframe by Average_MAE in ascending order
sorted_avg_mae <- avg_mae %>% arrange(Average_MAE)

sorted_avg_mae
```